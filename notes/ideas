change Symbolic to remove type families (e.g. Symbolic Rule instead of Symbolic (Rule f v))
take size of top term into account when ordering CPs

Use following idea from Bachmair:

  "For instance, suppose that all rewrite rules are numbered during
  completion in such a way that no two rules in any system Rn are
  assigned the same number. In addition, a rule is marked once all
  critical pairs with lower-numbered rules have been computed. Let P
  be a critical overlap s <- u -> t and supposed the rules numbered i,
  j, and k, respectively, are applied in the proof steps u -> s, u ->
  t and u -> v. If rule max(i, k) is marked, then all critical pairs
  between the corresponding rules have been computed, so that s and v
  are connected below u. Similarly, if rule max(j, k) is marked, then
  t and v are connected below u."

  As we eagerly add critical pairs, "rule max(i, k) is marked" means:
  rule i is the earlier rule of the two in the critical pair, and rule
  k is an earlier rule than the one from the critical pair. In particular,
  when we generate a critical pair, check that the top term can't be
  reduced by any earlier rule than the one we actually picked. (?)

Add "evil model" construction from before
(XXX does this help? It helped before when we would add conditional rules when a thing wasn't joinable. But now maybe it apes the model elimination thing we do anyway.)

Do narrowing

Implement unification indexing etc

Add proof production

Take age into account with critical pairs. Other ideas:
  * Number of CPs generated from same rule (the fewer the better)
  * Try each rule before going on to the next one
  * Reduce size if it matches goal term
    (subtract size of goal term from both sides?)

When generating a critical pair s <- t -> u,
add constraints s < t and t > u.

Try to combine as many axioms as possible, by recording which axioms
created each critical pair (including transitively) and minimising the
maximum number of occurrences of a single axiom

Try COL064-1 as a test case for unification indexing?

We often get laws like
  some very long term -> 0
which it seems can't often be useful (esp if 0 doesn't occur in the
goal). Can we work out which laws can actually help prove the goal?

Do we really need to eagerly generate critical pairs from recently
generated laws? Maybe gradually notch up maximum age of considered
critical pairs. E.g.: only consider CPs from rules up to label l/2,
where l is the current label. Perhaps print out the maximum CP age
actually used in a proof.

Goal-directed proving - boost CP whose RHS matches a subterm of a
goal, once rule is added add LHS as a goal

Suppose we have the rule
  x*x*x*x*x*x -> x
Then we should give the term
  x*x*x*x*y
a relatively small weight, because the instance [y->x*x] rewrites to x.
Somehow, terms that are "close to" being reduced to small terms should
get a smaller weight.

Prioritise CPs with BIG top terms? As this indicates a large
simplification step.

Look at size of overlap when weighting CPs (big overlap - better?)
Look at repeated variables (more repeated variables - better because
harder to form a small critical pair?)

Do we really need to rule out the case old `isInstanceOf` new in
subsumption checking? Doesn't it amount to doing case split on
direction of old rule. Possibly, but then we need to be more careful
when doing interreduction - at the moment we can interreduce two rules
using each other if we're not careful.

For QuickSpec-style "only generate laws up to this size": when a
critical pair is this size, join it under the constraint X <= c for
all variables X where c is a maximal constant of size 1. This effectively
forces all variables to have size one.

When we discover the crucial lemma, it causes bajillions of existing
rules to be discarded. Can we somehow use this as a similar/boringness
metric on these existing rules? (The fact that there is a single rule
which will generalise them all.)

Why does test/abelian --no-set-join terminate, and with such a weird
set of rules?
