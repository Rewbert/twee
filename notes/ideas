Make LPO go faster
Fix nontermination in case split
Critical pair improvement

penalise critical pairs that have repeated occurrences of the same
variable (too constrained). more generally: measure how widely
applicable the critical pair would be (how many terms match?
how many terms that are likely to appear in the proof match?)

Add proof production

Take age into account with critical pairs. Other ideas:
  * Number of CPs generated from same rule (the fewer the better)
  * Try each rule before going on to the next one
  * Reduce size if it matches goal term
    (subtract size of goal term from both sides?)

When generating a critical pair s <- t -> u,
add constraints s < t and t > u.

Try to combine as many axioms as possible, by recording which axioms
created each critical pair (including transitively) and minimising the
maximum number of occurrences of a single axiom

Do we really need to eagerly generate critical pairs from recently
generated laws? Maybe gradually notch up maximum age of considered
critical pairs. E.g.: only consider CPs from rules up to label l/2,
where l is the current label. Perhaps print out the maximum CP age
actually used in a proof.

Goal-directed proving - boost CP whose RHS matches a subterm of a
goal, once rule is added add LHS as a goal

Suppose we have the rule
  x*x*x*x*x*x -> x
Then we should give the term
  x*x*x*x*y
a relatively small weight, because the instance [y->x*x] rewrites to x.
Somehow, terms that are "close to" being reduced to small terms should
get a smaller weight.

Prioritise CPs with BIG top terms? As this indicates a large
simplification step.

Look at size of overlap when weighting CPs (big overlap - better?)
Look at repeated variables (more repeated variables - better because
harder to form a small critical pair?)

Do we really need to rule out the case old `isInstanceOf` new in
subsumption checking? Doesn't it amount to doing case split on
direction of old rule. Possibly, but then we need to be more careful
when doing interreduction - at the moment we can interreduce two rules
using each other if we're not careful.

For QuickSpec-style "only generate laws up to this size": when a
critical pair is this size, join it under the constraint X <= c for
all variables X where c is a maximal constant of size 1. This effectively
forces all variables to have size one.

When we discover the crucial lemma, it causes bajillions of existing
rules to be discarded. Can we somehow use this as a similar/boringness
metric on these existing rules? (The fact that there is a single rule
which will generalise them all.)

Why does test/abelian --no-set-join terminate, and with such a weird
set of rules?
